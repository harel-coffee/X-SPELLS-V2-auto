{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "anchor3",
      "language": "python",
      "name": "anchor3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "anchor_experiment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsiVplnxV4SZ",
        "outputId": "392fb24d-19a3-4919-e9d9-fadccdee7683"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "import os.path\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "import sklearn.linear_model\n",
        "import sklearn.ensemble\n",
        "import spacy\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "!pip install anchor_exp\n",
        "from anchor import anchor_text\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: anchor_exp in /usr/local/lib/python3.7/dist-packages (0.0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from anchor_exp) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from anchor_exp) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from anchor_exp) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from anchor_exp) (2.2.4)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.7/dist-packages (from anchor_exp) (0.2.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->anchor_exp) (1.0.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime->anchor_exp) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime->anchor_exp) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime->anchor_exp) (4.62.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (2.6.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime->anchor_exp) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime->anchor_exp) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime->anchor_exp) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime->anchor_exp) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime->anchor_exp) (1.15.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->anchor_exp) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6pq1JZFV4Sd"
      },
      "source": [
        "def cleanText(var):\n",
        "    # replace punctuation with spaces\n",
        "    var = re.sub('[{}]'.format(string.punctuation), \" \", var)\n",
        "    # remove double spaces\n",
        "    var = re.sub(r'\\s+', \" \", var)\n",
        "    # put in lower case\n",
        "    var = var.lower().split()\n",
        "    # remove words that are smaller than 3 characters\n",
        "    var = [w for w in var if len(w) >= 3]\n",
        "    var = \" \".join(var)\n",
        "    return var\n",
        "\n",
        "# Removes 'rt' from all input data\n",
        "def my_clean(text):\n",
        "    text = text.lower().split()\n",
        "    text = [w for w in text]\n",
        "    text = \" \".join(text)\n",
        "    text = re.sub(r\"rt\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Removes 'rt' from all input data\n",
        "# Removes emojis from all input data\n",
        "def YOUTUBE_my_clean(text):\n",
        "    text = text.lower().split()\n",
        "    text = [w for w in text]\n",
        "    text = \" \".join(text)\n",
        "    text = re.sub(r\"rt\", \"\", text)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = re.sub(emoji_pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def strip_links(text):\n",
        "    link_regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
        "    links = re.findall(link_regex, text)\n",
        "    for link in links:\n",
        "        text = text.replace(link[0], ', ')\n",
        "    return text\n",
        "\n",
        "\n",
        "def strip_all_entities(text):\n",
        "    entity_prefixes = ['@', '#']\n",
        "    for separator in string.punctuation:\n",
        "        if separator not in entity_prefixes:\n",
        "            text = text.replace(separator, ' ')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            if word[0] not in entity_prefixes:\n",
        "                words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def preProcessing(strings):\n",
        "    clean_tweet_texts = []\n",
        "    for string in strings:\n",
        "        clean_tweet_texts.append(my_clean(strip_all_entities(strip_links(string))))\n",
        "        # clean_tweet_texts.append(my_clean(string))\n",
        "    return clean_tweet_texts\n",
        "\n",
        "def YOUTUBE_preProcessing(strings):\n",
        "    clean_tweet_texts = []\n",
        "    for string in strings:\n",
        "        clean_tweet_texts.append(YOUTUBE_my_clean(strip_all_entities(strip_links(string))))\n",
        "        # clean_tweet_texts.append(my_clean(string))\n",
        "    return clean_tweet_texts\n",
        "\n",
        "def load_dataset(dataset):\n",
        "    if dataset == \"polarity\":\n",
        "        df = pd.read_csv('https://raw.githubusercontent.com/lstate/X-SPELLS-V2/main/data/' + dataset_name + '_tweets.csv', encoding='utf-8')\n",
        "        X = df['tweet'].values\n",
        "        y = df['class'].values\n",
        "\n",
        "    elif dataset == \"hate\":\n",
        "        df = pd.read_csv('https://raw.githubusercontent.com/lstate/X-SPELLS-V2/main/data/' + dataset_name + '_tweets.csv', encoding='utf-8')\n",
        "        # Removing the offensive comments, keeping only neutral and hatespeech,\n",
        "        # and convert the class value from 2 to 1 for simplification purposes\n",
        "        df = df[df['class'] != 1]\n",
        "        X = df['tweet'].values\n",
        "        y = df['class'].apply(lambda x: 1 if x == 2 else 0).values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.25)\n",
        "\n",
        "    X_test = preProcessing(X_test)\n",
        "\n",
        "    return X_test, y_test\n",
        "\n",
        "def YOUTUBE_get_text_data():\n",
        "    df = pd.read_csv('https://raw.githubusercontent.com/lstate/X-SPELLS-V2/main/data/YouTube-Spam-Collection-v1/youtube.csv', encoding='utf-8')\n",
        "\n",
        "    X = df[\"CONTENT\"].values\n",
        "    y = df[\"CLASS\"].values\n",
        "        \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.25)\n",
        "    \n",
        "    X_test = YOUTUBE_preProcessing(X_test)\n",
        "    \n",
        "    # delete x/y where there is no more content after preprocessing or we have more than 140 characters (e.g. comment was only an url)\n",
        "    \n",
        "    indx = []\n",
        "    for i in range(len(X_test)):\n",
        "        if len(X_test[i]) == 0:\n",
        "            indx.append(i)\n",
        "        elif len(X_test[i]) > 140:\n",
        "            indx.append(i)     \n",
        "    X_test = np.delete(X_test, indx, 0)\n",
        "    y_test = np.delete(y_test, indx, 0)\n",
        "    \n",
        "    return X_test, y_test"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaBYnn7bV4Si",
        "outputId": "af1de2cd-0c22-4853-a017-8cfe4a4c6a2c"
      },
      "source": [
        "!pip install spacy && python -m spacy download en_core_web_sm\n",
        "!pip install torch transformers spacy && python -m spacy download en_core_web_sm\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNQRMM4L2iCu"
      },
      "source": [
        "# Hate Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X5KSUWGV4Sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e00ede-2c43-469e-83c3-a88a17170ca7"
      },
      "source": [
        "dataset_name = 'hate'\n",
        "data, labels = load_dataset(dataset_name)\n",
        "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(train)\n",
        "train_vectors = vectorizer.transform(train)\n",
        "test_vectors = vectorizer.transform(test)\n",
        "\n",
        "c = sklearn.ensemble.RandomForestClassifier()\n",
        "c.fit(train_vectors, train_labels)\n",
        "\n",
        "preds = c.predict(test_vectors)\n",
        "\n",
        "print(classification_report(test_labels, preds))\n",
        "print(\"The accuracy score is {:.2%}\".format(accuracy_score(test_labels, preds)))\n",
        "\n",
        "def predict_lr(texts):\n",
        "    return c.predict(vectorizer.transform(texts))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.67      0.77        87\n",
            "           1       0.90      0.98      0.94       263\n",
            "\n",
            "    accuracy                           0.90       350\n",
            "   macro avg       0.90      0.82      0.85       350\n",
            "weighted avg       0.90      0.90      0.89       350\n",
            "\n",
            "The accuracy score is 90.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-UEpRnaV4TK"
      },
      "source": [
        "## Without using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhYjtRUBV4TP",
        "outputId": "3f521151-12bd-417e-b2e7-bbfb5499334f"
      },
      "source": [
        "explainer = anchor_text.AnchorText(nlp, ['hate-speech', 'neutral'], use_unk_distribution=True)\n",
        "\n",
        "np.random.seed(1)\n",
        "text = 'america is white trash and so are all americans'\n",
        "pred = explainer.class_names[predict_lr([text])[0]]\n",
        "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
        "exp = explainer.explain_instance(text, predict_lr, threshold=0.95)\n",
        "\n",
        "print('Text: %s' % text)\n",
        "print('Prediction: %s' % pred)\n",
        "print()\n",
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: america is white trash and so are all americans\n",
            "Prediction: hate-speech\n",
            "\n",
            "Anchor: white\n",
            "Precision: 1.00\n",
            "\n",
            "Examples where anchor applies and model predicts hate-speech:\n",
            "\n",
            "UNK UNK white trash UNK UNK UNK UNK UNK\n",
            "UNK UNK white UNK and so are all americans\n",
            "UNK is white trash and so are all americans\n",
            "UNK UNK white UNK UNK so are all americans\n",
            "UNK UNK white UNK UNK UNK UNK UNK americans\n",
            "america UNK white UNK UNK UNK are UNK americans\n",
            "america is white trash UNK so UNK UNK americans\n",
            "UNK is white trash UNK so UNK all americans\n",
            "america is white trash UNK so are UNK americans\n",
            "UNK is white trash UNK so are UNK UNK\n",
            "\n",
            "Examples where anchor applies and model predicts neutral:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxZfe94qV4TT"
      },
      "source": [
        "## Using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkQZ29XPV4TW",
        "outputId": "3d76557b-9721-4beb-cd66-7a317a1c018e"
      },
      "source": [
        "explainer = anchor_text.AnchorText(nlp, ['hate-speech', 'neutral'], use_unk_distribution=False)\n",
        "\n",
        "np.random.seed(1)\n",
        "text = 'america is white trash and so are all americans'\n",
        "pred = explainer.class_names[predict_lr([text])[0]]\n",
        "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
        "b = time.time()\n",
        "exp = explainer.explain_instance(text, predict_lr, threshold=0.95, verbose=False)\n",
        "print('Time: %s' % (time.time() - b))\n",
        "\n",
        "print('Text: %s' % text)\n",
        "print('Prediction: %s' % pred)\n",
        "print()\n",
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_different_prediction=True)]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time: 1666.1465530395508\n",
            "Text: america is white trash and so are all americans\n",
            "Prediction: hate-speech\n",
            "\n",
            "Anchor: white AND america AND trash AND is AND and\n",
            "Precision: 0.97\n",
            "\n",
            "Examples where anchor applies and model predicts hate-speech:\n",
            "\n",
            "america is white trash and residents are white trash\n",
            "america is white trash and therefore promotes humanity .\n",
            "america is white trash and a ##gro ##cer ##rado\n",
            "america is white trash and blacks are racist .\n",
            "america is white trash and blacks are blue trash\n",
            "america is white trash and trout are prohibited .\n",
            "america is white trash and operates these places :\n",
            "america is white trash and offers tuition courses .\n",
            "america is white trash and grass ##roots # #\n",
            "america is white trash and whites are black .\n",
            "\n",
            "Examples where anchor applies and model predicts neutral:\n",
            "\n",
            "america is white trash and poles are dry trash\n",
            "america is white trash and others are green trash\n",
            "america is white trash and sees what wants us\n",
            "america is white trash and we are nothing tomorrow\n",
            "america is white trash and buildings are clean ##able\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRHn0mZ04X2e"
      },
      "source": [
        "# Polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUupvE7z7lqK",
        "outputId": "8a1534bf-b5fc-4cf7-8953-da721165462b"
      },
      "source": [
        "dataset_name = 'polarity'\n",
        "data, labels = load_dataset(dataset_name)\n",
        "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(train)\n",
        "train_vectors = vectorizer.transform(train)\n",
        "test_vectors = vectorizer.transform(test)\n",
        "\n",
        "c = sklearn.ensemble.RandomForestClassifier()\n",
        "c.fit(train_vectors, train_labels)\n",
        "\n",
        "preds = c.predict(test_vectors)\n",
        "\n",
        "print(classification_report(test_labels, preds))\n",
        "print(\"The accuracy score is {:.2%}\".format(accuracy_score(test_labels, preds)))\n",
        "\n",
        "def predict_lr(texts):\n",
        "    return c.predict(vectorizer.transform(texts))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.70      0.67       339\n",
            "           1       0.66      0.61      0.63       328\n",
            "\n",
            "    accuracy                           0.65       667\n",
            "   macro avg       0.65      0.65      0.65       667\n",
            "weighted avg       0.65      0.65      0.65       667\n",
            "\n",
            "The accuracy score is 65.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXEaBQ2U7x3S"
      },
      "source": [
        "## Without using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lty84nOe72wB",
        "outputId": "754e5a75-ca34-4e22-a032-82ce0f9205a6"
      },
      "source": [
        "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)\n",
        "\n",
        "np.random.seed(1)\n",
        "text = 'eccentric enough to stave off doldrums caruso s self conscious debut is also eminently forgettable'\n",
        "pred = explainer.class_names[predict_lr([text])[0]]\n",
        "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
        "exp = explainer.explain_instance(text, predict_lr, threshold=0.95)\n",
        "\n",
        "print('Text: %s' % text)\n",
        "print('Prediction: %s' % pred)\n",
        "print()\n",
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: eccentric enough to stave off doldrums caruso s self conscious debut is also eminently forgettable\n",
            "Prediction: negative\n",
            "\n",
            "Anchor: forgettable\n",
            "Precision: 1.00\n",
            "\n",
            "Examples where anchor applies and model predicts negative:\n",
            "\n",
            "eccentric UNK UNK UNK off UNK caruso s self conscious UNK is also UNK forgettable\n",
            "eccentric enough UNK UNK off doldrums caruso s self UNK UNK is also UNK forgettable\n",
            "eccentric enough to UNK UNK doldrums caruso UNK UNK UNK debut UNK UNK eminently forgett\n",
            "eccentric UNK to stave UNK doldrums UNK UNK UNK UNK debut UNK also UNK forgettable\n",
            "eccentric enough to UNK UNK doldrums UNK UNK self conscious UNK is UNK eminently forget\n",
            "eccentric UNK UNK UNK UNK UNK UNK UNK self UNK debut UNK also eminently forgettable\n",
            "eccentric enough to stave off doldrums caruso s UNK UNK debut UNK UNK UNK forgettable\n",
            "eccentric UNK to stave off UNK caruso s UNK UNK UNK is UNK UNK forgettable\n",
            "UNK UNK UNK UNK off UNK UNK UNK self UNK UNK is UNK UNK forgettable\n",
            "eccentric UNK UNK stave off UNK caruso s UNK conscious debut UNK UNK eminently forgetta\n",
            "\n",
            "Examples where anchor applies and model predicts positive:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-WoEqB079BW"
      },
      "source": [
        "## Using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIazldh87_WN",
        "outputId": "d63ca28d-7738-467d-88fb-6307594e555d"
      },
      "source": [
        "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=False)\n",
        "\n",
        "np.random.seed(1)\n",
        "text = 'eccentric enough to stave off doldrums caruso s self conscious debut is also eminently forgettable'\n",
        "pred = explainer.class_names[predict_lr([text])[0]]\n",
        "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
        "b = time.time()\n",
        "exp = explainer.explain_instance(text, predict_lr, threshold=0.95, verbose=False)\n",
        "print('Time: %s' % (time.time() - b))\n",
        "\n",
        "print('Text: %s' % text)\n",
        "print('Prediction: %s' % pred)\n",
        "print()\n",
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_different_prediction=True)]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time: 1075.618047952652\n",
            "Text: eccentric enough to stave off doldrums caruso s self conscious debut is also eminently forgettable\n",
            "Prediction: negative\n",
            "\n",
            "Anchor: eccentric\n",
            "Precision: 1.00\n",
            "\n",
            "Examples where anchor applies and model predicts negative:\n",
            "\n",
            "eccentric ##о ##в п ##л ##ю ##ь л @ л ##л ##ь ##ь л л\n",
            "eccentric ρ ##ρ _ ρ # ρ ² ##δ W ##ᵐ ρ * ρ ##²\n",
            "eccentric ##φ = \\ off \\ φ \\ \\ φ \\ { φ } \\\n",
            "eccentric search to perform fusion energy to predicted orbital angular velocity causes orbital disturbance .\n",
            "eccentric ##υ ##ε ##λ ##ε # λ # λ # λ # κ ; radius\n",
            "eccentric ##म ##च ##ा ##ा # # # # # # # # # forgettable\n",
            "eccentric - to ##re ##versing ^ { bracket } var { segment } } {\n",
            "eccentric formula to obtain angular coordinates using spin space metric with formula _ 67 ⟩\n",
            "eccentric analogy to ∞ ##∞ # ∞ ∞ ##♦ ##∞ # # # # forgettable\n",
            "eccentric response to torque ##back ##rank # # # # # is ##abe ##aa #\n",
            "\n",
            "Examples where anchor applies and model predicts positive:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X222O_wvlDk6"
      },
      "source": [
        "# Youtube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aksrwWPklKkR",
        "outputId": "dc5510c7-2f5a-45dc-f19e-e251e81a4162"
      },
      "source": [
        "dataset_name = 'youtube'\n",
        "data, labels = YOUTUBE_get_text_data()\n",
        "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(train)\n",
        "train_vectors = vectorizer.transform(train)\n",
        "test_vectors = vectorizer.transform(test)\n",
        "\n",
        "c = sklearn.ensemble.RandomForestClassifier()\n",
        "c.fit(train_vectors, train_labels)\n",
        "\n",
        "preds = c.predict(test_vectors)\n",
        "\n",
        "print(classification_report(test_labels, preds))\n",
        "print(\"The accuracy score is {:.2%}\".format(accuracy_score(test_labels, preds)))\n",
        "\n",
        "def predict_lr(texts):\n",
        "    return c.predict(vectorizer.transform(texts))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91        55\n",
            "           1       0.95      0.82      0.88        45\n",
            "\n",
            "    accuracy                           0.90       100\n",
            "   macro avg       0.91      0.89      0.90       100\n",
            "weighted avg       0.90      0.90      0.90       100\n",
            "\n",
            "The accuracy score is 90.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wFS1NiCnf9_"
      },
      "source": [
        "## Without using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H46v6JennmUD",
        "outputId": "d536a622-8e1e-4b92-a800-880e80be69d4"
      },
      "source": [
        "explainer = anchor_text.AnchorText(nlp, ['no spam', 'spam'], use_unk_distribution=True)\n",
        "\n",
        "np.random.seed(1)\n",
        "text = 'check out this video on youtube'\n",
        "pred = explainer.class_names[predict_lr([text])[0]]\n",
        "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
        "exp = explainer.explain_instance(text, predict_lr, threshold=0.95)\n",
        "\n",
        "print('Text: %s' % text)\n",
        "print('Prediction: %s' % pred)\n",
        "print()\n",
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: check out this video on youtube\n",
            "Prediction: spam\n",
            "\n",
            "Anchor: out\n",
            "Precision: 0.98\n",
            "\n",
            "Examples where anchor applies and model predicts spam:\n",
            "\n",
            "UNK out this video on UNK\n",
            "check out UNK UNK UNK youtube\n",
            "UNK out this video UNK UNK\n",
            "UNK out this video UNK youtube\n",
            "check out UNK video on youtube\n",
            "UNK out this video UNK youtube\n",
            "UNK out this UNK on UNK\n",
            "check out UNK UNK on youtube\n",
            "UNK out this video on UNK\n",
            "UNK out this video UNK youtube\n",
            "\n",
            "Examples where anchor applies and model predicts no spam:\n",
            "\n",
            "UNK out UNK UNK UNK UNK\n",
            "UNK out UNK UNK UNK UNK\n",
            "UNK out UNK UNK UNK UNK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_v9qbbNnoqR"
      },
      "source": [
        "## Using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTkZNNnLnrRU",
        "outputId": "c939a454-470c-442d-a6f3-914f42b673aa"
      },
      "source": [
        "explainer = anchor_text.AnchorText(nlp, ['no spam', 'spam'], use_unk_distribution=False)\n",
        "\n",
        "np.random.seed(1)\n",
        "text = 'check out this video on youtube'\n",
        "pred = explainer.class_names[predict_lr([text])[0]]\n",
        "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
        "b = time.time()\n",
        "exp = explainer.explain_instance(text, predict_lr, threshold=0.95, verbose=False)\n",
        "print('Time: %s' % (time.time() - b))\n",
        "\n",
        "print('Text: %s' % text)\n",
        "print('Prediction: %s' % pred)\n",
        "print()\n",
        "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
        "print('Precision: %.2f' % exp.precision())\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % pred)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
        "print()\n",
        "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
        "print()\n",
        "print('\\n'.join([x[0] for x in exp.examples(only_different_prediction=True)]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time: 196.63774037361145\n",
            "Text: check out this video on youtube\n",
            "Prediction: spam\n",
            "\n",
            "Anchor: out AND youtube\n",
            "Precision: 1.00\n",
            "\n",
            "Examples where anchor applies and model predicts spam:\n",
            "\n",
            "cut out the video on youtube\n",
            "check out this page on youtube\n",
            "check out movie trailers on youtube\n",
            "file out play ##list on youtube\n",
            "get out loud online on youtube\n",
            "check out this name on youtube\n",
            "check out from track manager youtube\n",
            "check out words live on youtube\n",
            "lock out - mobile animated youtube\n",
            "burning out the sun \" youtube\n",
            "\n",
            "Examples where anchor applies and model predicts no spam:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
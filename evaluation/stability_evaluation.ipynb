{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabiltiy Evaluation\n",
    "\n",
    "Same script for distance/diversity and BVAE/OVAE (BVAE here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stability Evaluation\n",
    "\n",
    "Uses knn to first predict the class of the given instance using the synthetic exemplars and counter exemplars\n",
    "and secondly using real sentences from the train set of the respective dataset.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from statistics import stdev\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from pre_processing import get_text_data, YOUTUBE_get_text_data\n",
    "\n",
    "from DNN_base import TextsToSequences, Padder, create_model\n",
    "\n",
    "sequencer = TextsToSequences(num_words=35000)\n",
    "padder = Padder(140)\n",
    "myModel = KerasClassifier(build_fn=create_model, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_k_sentences(sentences, ids, k, metric):\n",
    "    index_list = ids\n",
    "    final_idx_distances = list()\n",
    "    sentences = [sentences[x] for x in ids]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentences_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    dictionary = dict(zip(index_list, sentences_vectors))\n",
    "    distances = [[] for _ in range(len(sentences))]\n",
    "    distances_dict = [dict() for _ in range(len(sentences))]\n",
    "    cosine_distance_list = [[] for _ in range(len(sentences))]\n",
    "    idx_distances = list()\n",
    "    count = 0\n",
    "    for idx in index_list:\n",
    "        instance = dictionary.get(idx)\n",
    "        instance = np.array(instance)\n",
    "\n",
    "        for j in index_list:\n",
    "            temp_state_sentence = dictionary.get(j)\n",
    "\n",
    "            distances[count].append(\n",
    "                cdist(instance.reshape(1, -1), temp_state_sentence.reshape(1, -1), metric='cosine').ravel())\n",
    "            idx_distances.append(j)\n",
    "\n",
    "        distances_dict[count] = dict(zip(idx_distances, distances[count]))\n",
    "        distances_sorted = {k: v for k, v in sorted(distances_dict[count].items(), key=lambda x: x[1])}\n",
    "        final_idxs, final_dists = zip(*list(distances_sorted.items()))\n",
    "        final_idx_distances.append(final_idxs[1:k + 1])\n",
    "\n",
    "        for j in range(1, closest_k + 1):\n",
    "            cosine_distance_list[count].append((final_dists[j] - final_dists[0])[0])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return index_list, final_idx_distances, cosine_distance_list\n",
    "\n",
    "\n",
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a.union(b)))\n",
    "\n",
    "\n",
    "def create_lime_explanation_words():\n",
    "    top_lime_words = list()\n",
    "    for i in loaded_ids:\n",
    "        print(i)\n",
    "        print(X[i])\n",
    "        # print(y_original[i])\n",
    "        split_expression = lambda s: re.split(r'\\W+', s)\n",
    "        explanation = explainer.explain_instance(X[i], c.predict_proba, num_features=5)\n",
    "        print('Probability(neutral) =', c.predict_proba([X[i]])[0, 1])\n",
    "        weights = OrderedDict(explanation.as_list())\n",
    "        print(list(weights.keys()))\n",
    "        top_lime_words.append(list(weights.keys()))\n",
    "        lime_w = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})\n",
    "        print(lime_w)\n",
    "\n",
    "    print(top_lime_words)\n",
    "\n",
    "    with open('data/' + datasetName + '_' + modelName + '_' + 'lime_top_words', 'wb') as f:\n",
    "        pickle.dump(top_lime_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasetNames = [\"youtube\", \"hate\", \"polarity\"]\n",
    "modelNames = [\"RF\", \"DNN\"]\n",
    "methods = [\"lime\", \"xspells\"]\n",
    "closest_k = 10  # How big the neighborhood should be in senteces\n",
    "\n",
    "for k, modelName in enumerate(modelNames):\n",
    "    for l, method in enumerate(methods):\n",
    "        for j, datasetName in enumerate(datasetNames):\n",
    "            if datasetName == \"youtube\":\n",
    "                class_names = ['no spam', 'spam']\n",
    "                _, _, _, y, _, X = YOUTUBE_get_text_data('data/YouTube-Spam-Collection-v1/' + datasetName + '.csv', datasetName)\n",
    "\n",
    "            if datasetName == \"polarity\":\n",
    "                class_names = ['negative', 'positive']\n",
    "                _, _, _, y, _, X = get_text_data('data/' + datasetName + '_tweets.csv', datasetName)\n",
    "\n",
    "            if datasetName == \"hate\":\n",
    "                class_names = ['hate-speech', 'neutral']\n",
    "                _, _, _, y, _, X = get_text_data('data/' + datasetName + '_tweets.csv', datasetName)\n",
    "\n",
    "                \n",
    "            # we should load two types of data: distance and diversity\n",
    "            with open(DATAPATH + datasetName + '_' + modelName + '_' + 'ids', 'rb') as f:\n",
    "                loaded_ids = pickle.load(f)\n",
    "\n",
    "            with open(DATAPATH + datasetName + '_' + modelName + '_' + 'top_exemplar_words', 'rb') as f:\n",
    "                loaded_top_exemplar_words = pickle.load(f)\n",
    "\n",
    "            with open(DATAPATH + datasetName + '_' + modelName + '_' + 'top_counter_exemplar_words', 'rb') as f:\n",
    "                loaded_top_counter_exemplar_words = pickle.load(f)\n",
    "\n",
    "            with open(DATAPATH + datasetName + '_' + modelName + '_' + 'predictions', 'rb') as f:\n",
    "                loaded_predictions = pickle.load(f)\n",
    "\n",
    "            '''Find closest k sentences for final experiment'''\n",
    "            index, closest_indexes, cosine_distance_list = (find_closest_k_sentences(X, loaded_ids,\n",
    "                                                                                     k=closest_k, metric='euclidean'))\n",
    "\n",
    "            closest_indexes_dict = dict(zip(index, closest_indexes))\n",
    "\n",
    "            pickled_black_box_filename = 'models/' + datasetName + '_saved_' + modelName + '_model.sav'\n",
    "\n",
    "            if modelName == \"RF\":\n",
    "                pickled_vectorizer_filename = 'models/' + datasetName + '_tfidf_vectorizer.pickle'\n",
    "                loaded_vectorizer = pickle.load(open(pickled_vectorizer_filename, 'rb'))\n",
    "            elif modelName == \"DNN\":\n",
    "                pickled_vectorizer_filename = None\n",
    "\n",
    "            loaded_model = pickle.load(open(pickled_black_box_filename, 'rb'))\n",
    "\n",
    "            if modelName is 'DNN':\n",
    "                # Use following if DNN\n",
    "                c = loaded_model\n",
    "            else:\n",
    "                # Use following if RF\n",
    "                c = make_pipeline(loaded_vectorizer, loaded_model)\n",
    "\n",
    "            print(\"model loaded\")\n",
    "\n",
    "            if method is 'xspells':\n",
    "                top_words_dict = dict(zip(loaded_ids, loaded_top_exemplar_words))\n",
    "            else:\n",
    "                explainer = LimeTextExplainer(class_names=class_names)\n",
    "                create_lime_explanation_words()\n",
    "\n",
    "                with open('data/' + datasetName + '_' + modelName + '_' + 'lime_top_words', 'rb') as f:\n",
    "                    loaded_top_lime_words = pickle.load(f)\n",
    "\n",
    "                top_words_dict = dict(zip(loaded_ids, loaded_top_lime_words))\n",
    "\n",
    "            print(\"explanation loaded/created\")\n",
    "\n",
    "            jaccard_distance_list = [[] for _ in range(len(loaded_ids))]\n",
    "            counter = 0\n",
    "\n",
    "            for i in loaded_ids:\n",
    "                tempList = list()\n",
    "                instance = ' '.join(map(str, top_words_dict.get(i)))\n",
    "\n",
    "                for j in range(closest_k):\n",
    "                    listToStr = ' '.join(map(str, top_words_dict.get(closest_indexes_dict[i][j])))\n",
    "                    tempList.append(listToStr)\n",
    "\n",
    "                for j in range(closest_k):\n",
    "                    jaccard_distance_list[counter].append(1 - get_jaccard_sim(instance, tempList[j]))\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "            instability_list = list()\n",
    "            for i in range(len(jaccard_distance_list)):\n",
    "                v1 = jaccard_distance_list[i][0] / cosine_distance_list[i][0]\n",
    "                vk = jaccard_distance_list[i][closest_k - 1] / cosine_distance_list[i][closest_k - 1]\n",
    "                # v1 = jaccard_distance_list[i][0]\n",
    "                # vk = jaccard_distance_list[i][closest_k - 1]\n",
    "                if (v1 / vk) != np.inf:\n",
    "                    instability_list.append(v1 / vk)\n",
    "\n",
    "            print(datasetName, modelName, method)\n",
    "            print('Average instability: ', (np.nansum(instability_list) / len(instability_list)))\n",
    "            print('Standard Deviation: ', np.nanstd(instability_list))\n",
    "            print(\"SUM\", len(instability_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env_3 clone",
   "language": "python",
   "name": "clone_xspells_conda_env_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

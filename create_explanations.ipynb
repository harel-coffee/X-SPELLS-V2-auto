{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create explanations\n",
    "\n",
    "Distance and diversity, BVAE and OVAE (here BVAE).\n",
    "\n",
    "For the OVAE, we can directly load the created sentencenes into the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from statistics import stdev\n",
    "\n",
    "import numpy as np\n",
    "import pydot_ng as pydot\n",
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import decision_tree\n",
    "from lstm_vae import inference\n",
    "\n",
    "pydot.find_graphviz()\n",
    "import csv\n",
    "import VAE_train.youtube_train_vae\n",
    "import VAE_train.train_BVAE\n",
    "from BB_train.DNN_base import TextsToSequences, Padder, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = TextsToSequences(num_words=35000)\n",
    "padder = Padder(140)\n",
    "myModel = KerasClassifier(build_fn=create_model, epochs=100)\n",
    "\n",
    "# below functions are needed for diversity optimization\n",
    "\n",
    "\"\"\" argmax function \"\"\"\n",
    "def argmax(values, f):\n",
    "    bestf = None\n",
    "    bestp = None\n",
    "    for i, v in enumerate(values):\n",
    "        fv = f(v)\n",
    "        if bestf is None or bestf < fv:\n",
    "            bestf =fv\n",
    "            bestp = i\n",
    "    return bestp, bestf\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import heapq\n",
    "\n",
    "# euclidian\n",
    "def dist(a, b):\n",
    "    return distance.euclidean(a, b)\n",
    "\n",
    "\"\"\" accelerated max cover \"\"\"\n",
    "# return argmax_{x subseteq aset} f(x) using accelerated greedy algorithm\n",
    "def acover(aset, f=None, k=None, df=None, verbose=True):\n",
    "    goal_function = []\n",
    "    if k is None or k >= len(aset):\n",
    "        k = len(aset)\n",
    "    # priority queue\n",
    "    if df is None:\n",
    "        pset = [ Pair(-f([v]), v) for v in aset]\n",
    "    else:\n",
    "        pset = [ Pair(-df([], v), v) for v in aset]\n",
    "    heapq.heapify(pset)\n",
    "    # covers is accumulated greedy set\n",
    "    covers = []\n",
    "    for i in range(k):\n",
    "        # get next value\n",
    "        best = heapq.heappop(pset)\n",
    "        best.a = -f(covers + [best.b]) if df is None else -df(covers, best.b)\n",
    "        # and check if it is better than any bound \n",
    "        newbest=heapq.heappushpop(pset, best)\n",
    "        while newbest.a < best.a:\n",
    "            best = newbest\n",
    "            best.a = -f(covers + [best.b]) if df is None else -df(covers, best.b)\n",
    "            newbest=heapq.heappushpop(pset, best)\n",
    "        # add it to the accumulated greedy set\n",
    "        if df is not None and best.a >= 0:\n",
    "                break\n",
    "        covers.append(best.b)\n",
    "        if verbose:\n",
    "            if df is None:\n",
    "                print('it:', i, 'x:', best.b, 'f:', -best.a)\n",
    "            else:\n",
    "                print('it:', i, 'x:', best.b, 'f:', df(covers), 'df:', -best.a)\n",
    "        # safe the values of the goal function \n",
    "        # df needs to be not None\n",
    "        goal_function.append(df(covers))\n",
    "    return covers, goal_function\n",
    "\n",
    "\"\"\" pairs of objects \"\"\"\n",
    "class Pair:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    \"\"\" lexicographic ordering \"\"\"\n",
    "    def __lt__(self, other):\n",
    "        return self.a < other.a or (self.a == other.a and self.b < other.b)\n",
    "    \n",
    "    \"\"\" lexicographic ordering \"\"\"\n",
    "    def __le__(self, other):\n",
    "        return self.a < other.a or (self.a == other.a and self.b <= other.b)\n",
    "    \n",
    "    \"\"\" lexicographic ordering \"\"\"\n",
    "    def __eq__(self, other):\n",
    "        return self.a == other.a and self.b == other.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_VAE(dataset_name):\n",
    "    vae = load_model(DATAPATH + dataset_name + '_vae_model.h5', compile=False)\n",
    "    enc = load_model(DATAPATH + dataset_name + '_enc_model.h5', compile=False)\n",
    "    gen = load_model(DATAPATH + dataset_name + '_gen_model.h5', compile=False)\n",
    "    stepper = load_model(DATAPATH + dataset_name + '_stepper_model.h5', compile=False)\n",
    "    vae.summary()\n",
    "    return vae, enc, gen, stepper\n",
    "\n",
    "\n",
    "def decode(s):\n",
    "    return inference.decode_sequence(s, gen, stepper, input_dim, char2id, id2char, max_encoder_seq_length)\n",
    "\n",
    "def get_sentences():\n",
    "    input_sentences = []\n",
    "    state_input_sentences = []\n",
    "    decoded_sentences = []\n",
    "\n",
    "    for i in range(len(encoder_input_data)):\n",
    "        mean, variance = enc.predict([[encoder_input_data[i]]])\n",
    "        seq = np.random.normal(size=(latent_dim,))\n",
    "        seq = mean + variance * seq\n",
    "        input_sentences.append(X_original_processed[i])\n",
    "        state_input_sentences.append(seq)\n",
    "        # decoded_sentences.append(decode(seq))\n",
    "\n",
    "    # return input_sentences, state_input_sentences, decoded_sentences\n",
    "    return input_sentences, state_input_sentences\n",
    "\n",
    "\n",
    "def calculate_MRE():\n",
    "    train_input_sentences = []\n",
    "    train_decoded_sentences = []\n",
    "\n",
    "    for i in range(int(len(encoder_input_data))):\n",
    "        print(i)\n",
    "        mean, variance = enc.predict([[encoder_input_data[i]]])\n",
    "        seq = np.random.normal(size=(latent_dim,))\n",
    "        #seq = mean\n",
    "        seq = mean + variance * seq\n",
    "        print('original: ', X_original_processed[i])\n",
    "        print('reconstructed: ', decode(seq))\n",
    "        train_input_sentences.append(X_original_processed[i])\n",
    "        train_decoded_sentences.append(decode(seq))\n",
    "\n",
    "    train_sentences_dict = dict(zip(train_input_sentences, train_decoded_sentences))\n",
    "    train_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "    train_sentences_vectors = train_vectorizer.fit_transform(train_input_sentences).toarray()\n",
    "    train_decoded_sentences_vectors = train_vectorizer.transform(train_decoded_sentences).toarray()\n",
    "\n",
    "    train_cosine_distance_list = list()\n",
    "\n",
    "    for i in range(len(train_sentences_vectors)):\n",
    "        train_cosine_distance_list.append((cdist(train_sentences_vectors[i].reshape(1, -1),\n",
    "                                                 train_decoded_sentences_vectors[i].reshape(1, -1),\n",
    "                                                 metric='cosine').ravel())[0])\n",
    "\n",
    "    print(train_sentences_dict)\n",
    "    print(train_cosine_distance_list)\n",
    "    print(\"MRE train: \", np.nansum(train_cosine_distance_list) / len(train_cosine_distance_list))\n",
    "    print(\"MRE train stdev: \", np.std(train_cosine_distance_list))\n",
    "\n",
    "\n",
    "def calculate_min_max(list):\n",
    "    c = np.min(list, axis=0)\n",
    "    d = np.max(list, axis=0)\n",
    "    return c, d\n",
    "\n",
    "def generate_sentences(number_of_sentences, number_of_max_attempts, number_of_random_sentences, probability):\n",
    "    state_sentences = [[] for _ in range(number_of_sentences)]\n",
    "    decoded_sentences = [[] for _ in range(number_of_sentences)]\n",
    "\n",
    "    for i in range(number_of_sentences):\n",
    "\n",
    "        print(\"sentence : \", i)\n",
    "        seq_from = latent_space_state[i]\n",
    "\n",
    "        number_of_ticks = 0\n",
    "        max_attempts = number_of_max_attempts\n",
    "        random_sentences_to_create = number_of_random_sentences\n",
    "\n",
    "        while (len(decoded_sentences[i]) < random_sentences_to_create) and number_of_ticks < max_attempts:\n",
    "\n",
    "            newseq = np.copy(seq_from)\n",
    "            for d in range(latent_dim):\n",
    "                rm = np.random.random()\n",
    "                if rm >= probability:\n",
    "                    newseq[0, d] = (largest_x[0, d] - smallest_x[0, d]) * np.random.random() + smallest_x[0, d]\n",
    "\n",
    "            if decode(newseq) not in decoded_sentences[i]:\n",
    "                state_sentences[i].append(newseq)\n",
    "                decoded_sentences[i].append(decode(newseq))\n",
    "\n",
    "            print(len(decoded_sentences[i]))\n",
    "            number_of_ticks += 1\n",
    "            print(number_of_ticks)\n",
    "\n",
    "    return state_sentences, decoded_sentences\n",
    "\n",
    "def get_predictions(bb_filename, vect_filename, number_of_sentences):\n",
    "    # Load the black box model from disk\n",
    "    filename = bb_filename\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    preds = [[] for _ in range(number_of_sentences)]\n",
    "\n",
    "    print('predictions are: ')\n",
    "\n",
    "    final_unique_state_sentences = [[] for _ in range(number_of_sentences)]  # with initial sentence on first place\n",
    "    final_unique_decoded_sentences = [[] for _ in range(number_of_sentences)]\n",
    "\n",
    "    if vect_filename is None:\n",
    "        for i in range(number_of_sentences):\n",
    "            final_unique_state_sentences[i].append(latent_space_state[i])\n",
    "            final_unique_decoded_sentences[i].append(in_sentences[i])\n",
    "\n",
    "            final_unique_state_sentences[i].extend(generated_state_sentences[i])\n",
    "            final_unique_decoded_sentences[i].extend(generated_decoded_sentences[i])\n",
    "\n",
    "            # Predicting\n",
    "            preds[i] = loaded_model.predict(final_unique_decoded_sentences[i])\n",
    "    else:\n",
    "        # Load the TF-IDF vectorizer of the respective dataset\n",
    "        vectorizer = pickle.load(open(vect_filename, 'rb'))\n",
    "        test_vectors = [[] for _ in range(number_of_sentences)]\n",
    "\n",
    "        for i in range(number_of_sentences):\n",
    "            final_unique_state_sentences[i].append(latent_space_state[i])\n",
    "            final_unique_decoded_sentences[i].append(in_sentences[i])\n",
    "\n",
    "            final_unique_state_sentences[i].extend(generated_state_sentences[i])\n",
    "            final_unique_decoded_sentences[i].extend(generated_decoded_sentences[i])\n",
    "            test_vectors[i] = vectorizer.transform(final_unique_decoded_sentences[i])\n",
    "                        \n",
    "            # Predicting\n",
    "            preds[i] = loaded_model.predict(test_vectors[i])\n",
    "\n",
    "    return preds, final_unique_state_sentences, final_unique_decoded_sentences\n",
    "\n",
    "\n",
    "def find_closest_k_latent_sentences(state_sentences, decoded_sentences, predictions, k):\n",
    "    negative_distances = list()\n",
    "    negative_idx_distances = list()\n",
    "    positive_distances = list()\n",
    "    positive_idx_distances = list()\n",
    "    negative_state_sentences = list()\n",
    "    positive_state_sentences = list()\n",
    "    negative_decoded_sentences = list()\n",
    "    positive_decoded_sentences = list()\n",
    "    negative_predictions = list()\n",
    "    positive_predictions = list()\n",
    "    instance_state_sentence = state_sentences[0]\n",
    "    instance_decoded_sentence = decoded_sentences[0]\n",
    "    instance_prediction = predictions[0]\n",
    "\n",
    "    for i in range(1, len(state_sentences)):\n",
    "        if predictions[i] == 0:\n",
    "            negative_state_sentences.append(state_sentences[i])\n",
    "            negative_decoded_sentences.append(decoded_sentences[i])\n",
    "            negative_predictions.append((predictions[i]))\n",
    "        else:\n",
    "            positive_state_sentences.append(state_sentences[i])\n",
    "            positive_decoded_sentences.append(decoded_sentences[i])\n",
    "            positive_predictions.append((predictions[i]))\n",
    "\n",
    "    for i in range(len(negative_state_sentences)):\n",
    "        negative_idx_distances.append(i)\n",
    "        negative_distances.append(cdist(instance_state_sentence.reshape(1, -1),\n",
    "                                        negative_state_sentences[i].reshape(1, -1), metric='euclidean').ravel())\n",
    "\n",
    "    for i in range(len(positive_state_sentences)):\n",
    "        positive_idx_distances.append(i)\n",
    "        positive_distances.append(cdist(instance_state_sentence.reshape(1, -1),\n",
    "                                        positive_state_sentences[i].reshape(1, -1), metric='euclidean').ravel())\n",
    "\n",
    "    negative_distances_dict = dict(zip(negative_idx_distances, negative_distances))\n",
    "    negative_distances_sorted = {k: v for k, v in sorted(negative_distances_dict.items(), key=lambda x: x[1])}\n",
    "\n",
    "    negative_final_idxs, negative_final_dists = zip(*list(negative_distances_sorted.items()))\n",
    "    negative_final_state_sentences = [negative_state_sentences[x] for x in negative_final_idxs[:100]]\n",
    "    negative_final_decoded_sentences = [negative_decoded_sentences[x] for x in negative_final_idxs[:100]]\n",
    "    negative_final_predictions = [negative_predictions[x] for x in negative_final_idxs[:100]]\n",
    "\n",
    "    positive_distances_dict = dict(zip(positive_idx_distances, positive_distances))\n",
    "    positive_distances_sorted = {k: v for k, v in sorted(positive_distances_dict.items(), key=lambda x: x[1])}\n",
    "\n",
    "    positive_final_idxs, positive_final_dists = zip(*list(positive_distances_sorted.items()))\n",
    "    positive_final_state_sentences = [positive_state_sentences[x] for x in positive_final_idxs[:100]]\n",
    "    positive_final_decoded_sentences = [positive_decoded_sentences[x] for x in positive_final_idxs[:100]]\n",
    "    positive_final_predictions = [positive_predictions[x] for x in positive_final_idxs[:100]]\n",
    "\n",
    "    return [instance_state_sentence] + negative_final_state_sentences + positive_final_state_sentences, \\\n",
    "           [instance_decoded_sentence] + negative_final_decoded_sentences + positive_final_decoded_sentences, \\\n",
    "           [instance_prediction] + negative_final_predictions + positive_final_predictions\n",
    "\n",
    "\n",
    "def default_kernel(d, kernel_width):\n",
    "    return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "\n",
    "def calculate_weights(Z, metric):\n",
    "    if np.max(Z) != 1 and np.min(Z) != 0:\n",
    "        Zn = (Z - np.min(Z)) / (np.max(Z) - np.min(Z))\n",
    "        distances = cdist(Zn, Zn[0].reshape(1, -1), metric=metric).ravel()\n",
    "    else:\n",
    "        distances = cdist(Z, Z[0].reshape(1, -1), metric=metric).ravel()\n",
    "\n",
    "    weights = kernel(distances)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def find_exemplars(Z, idxs, metric):\n",
    "    distances = list()\n",
    "    idx_distances = list()\n",
    "    for t in idxs:\n",
    "        distances.append(cdist(Z[0].reshape(1, -1), Z[t].reshape(1, -1), metric=metric).ravel())\n",
    "        idx_distances.append(t)\n",
    "\n",
    "    distances_dict = dict(zip(idx_distances, distances))\n",
    "    distances_sorted = {k: v for k, v in sorted(distances_dict.items(), key=lambda x: x[1])}\n",
    "    final_idxs, final_dists = zip(*list(distances_sorted.items()))\n",
    "\n",
    "    return final_idxs[1:]\n",
    "\n",
    "\n",
    "def find_counter_exemplars(Z, idxs, metric, count):\n",
    "    distances = list()\n",
    "    idx_distances = list()\n",
    "    for t in idxs:\n",
    "        distances.append(cdist(Z[0].reshape(1, -1), Z[t].reshape(1, -1), metric=metric).ravel())\n",
    "        idx_distances.append(t)\n",
    "\n",
    "    distances_dict = dict(zip(idx_distances, distances))\n",
    "    distances_sorted = {k: v for k, v in sorted(distances_dict.items(), key=lambda x: x[1])}\n",
    "    final_idxs, final_dists = zip(*list(distances_sorted.items()))\n",
    "    \n",
    "    return final_idxs[:count]\n",
    "\n",
    "def EVAL_find_counter_exemplars(latent_representation_original, Z, idxs, counter_exemplar_idxs):\n",
    "    \"\"\"\n",
    "    Compute the values of the goal function.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare the data to apply the diversity optimization \n",
    "    data = np.zeros((len(idxs), np.shape(Z)[1]))\n",
    "    for i in range(len(idxs)):\n",
    "        data[i] = Z[idxs[i]]            \n",
    "        \n",
    "    # min-max normalization (applied on ALL examples)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit_transform(data)\n",
    "    \n",
    "    # list of points\n",
    "    points = [row for row in scaler.transform(data)]\n",
    "    # MIN MAX normalize instance to explain\n",
    "    instance = scaler.transform((latent_representation_original))\n",
    "    \n",
    "    # number of nearest neighbors to consider    \n",
    "    knn = 5\n",
    "    \n",
    "    kp = {}\n",
    "    lconst = 1\n",
    "    _, d0 =  argmax(points, lambda p: -dist(instance, p))\n",
    "    lconst = 0.5/(-d0)\n",
    "    \n",
    "    for p1 in points:\n",
    "        # compute distances\n",
    "        dst = [(p2, dist(p1, p2)) for p2 in points if not np.array_equal(p1, p2)]\n",
    "        # sort\n",
    "        dst = sorted(dst, key=lambda x: x[1])\n",
    "        # add top knn to kp\n",
    "        kp[p1.tobytes()] = set(p2.tobytes() for p2, d in dst[:knn]) \n",
    "        \n",
    "    # goal function    \n",
    "    def g(points):\n",
    "        dpoints, dx = set(), 0\n",
    "        for p1 in points:\n",
    "            # union operator\n",
    "            dpoints |= kp[p1.tobytes()]\n",
    "            dx += dist(p1, instance)\n",
    "         # scaled version 2*cost\n",
    "        return len(dpoints) - 2 * lconst * dx\n",
    "    \n",
    "    # get the extracted CF\n",
    "    extracted_CF_data = []\n",
    "    for i in range(len(counter_exemplar_idxs)):\n",
    "        extracted_CF_data.append(Z[counter_exemplar_idxs[i]]) \n",
    "        \n",
    "    # apply scaling\n",
    "    extracted_CF_data = scaler.transform((extracted_CF_data))\n",
    "    \n",
    "    return g(extracted_CF_data)\n",
    "\n",
    "        \n",
    "\n",
    "def DIVERSITY_find_counter_exemplars(latent_representation_original, Z, idxs, metric, count):\n",
    "    \"\"\"\n",
    "    Pick CF based on diversity optimization.\n",
    "    Additionally, the consecutive values of the goal function are returned\n",
    "    \"\"\"\n",
    "        \n",
    "    # prepare the data to apply the diversity optimization \n",
    "    data = np.zeros((len(idxs), np.shape(Z)[1]))\n",
    "    for i in range(len(idxs)):\n",
    "        data[i] = Z[idxs[i]]            \n",
    "        \n",
    "    # min-max normalization (applied on ALL examples)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit_transform(data)\n",
    "    \n",
    "    # list of points\n",
    "    points = [row for row in scaler.transform(data)]\n",
    "    # MIN MAX normalize instance to explain\n",
    "    instance = scaler.transform((latent_representation_original))\n",
    "        \n",
    "    # number of nearest neighbors to consider    \n",
    "    knn = 5\n",
    "    \n",
    "    kp = {}\n",
    "    lconst = 1\n",
    "    _, d0 =  argmax(points, lambda p: -dist(instance, p))\n",
    "    lconst = 0.5/(-d0)\n",
    "    \n",
    "    for p1 in points:\n",
    "        # compute distances\n",
    "        dst = [(p2, dist(p1, p2)) for p2 in points if not np.array_equal(p1, p2)]\n",
    "        # sort\n",
    "        dst = sorted(dst, key=lambda x: x[1])\n",
    "        # add top knn to kp\n",
    "        kp[p1.tobytes()] = set(p2.tobytes() for p2, d in dst[:knn]) \n",
    "        \n",
    "    # goal function    \n",
    "    def g(points):\n",
    "        dpoints, dx = set(), 0\n",
    "        for p1 in points:\n",
    "            # union operator\n",
    "            dpoints |= kp[p1.tobytes()]\n",
    "            dx += dist(p1, instance)\n",
    "         # scaled version 2*cost\n",
    "        return len(dpoints) - 2 * lconst * dx\n",
    "        \n",
    "    gderiv = lambda p, x=None: g(p) if x is None else g(p+[x])-g(p)\n",
    "\n",
    "    # can use acover \n",
    "    # cover holds the points chosen by the algorithm\n",
    "    # g_values holds the consecutive values of the goal function\n",
    "    covers, g_values = acover(points, df=gderiv, k=count, verbose=False)\n",
    "    \n",
    "    # from list back to matrix\n",
    "    cov = np.concatenate([ [a] for a in covers])\n",
    "    # and rescale to original coordinates\n",
    "    cov = scaler.inverse_transform(cov)\n",
    "    \n",
    "    # get back the indices of the elements that are chosen by the greedy algorithm\n",
    "    \n",
    "    data_indices = []\n",
    "    \n",
    "    for i in range(len(cov)):\n",
    "        for j in range(len(data)):\n",
    "            if np.allclose(cov[i], data[j]):\n",
    "                data_indices.append(j)\n",
    "            \n",
    "    # convert into CF indices so that we can return a list that holds only CF elements, generated by greedy alg\n",
    "    \n",
    "    final_indices = []\n",
    "    \n",
    "    for i in range(len(data_indices)):\n",
    "        final_indices.append(idxs[data_indices[i]])\n",
    "        \n",
    "    # get back the original data\n",
    "    # print(Z[final_indices[i]])\n",
    "\n",
    "    return final_indices, g_values\n",
    "\n",
    "\n",
    "def find_most_common_words(A, n):\n",
    "    split_words = ' '.join(A).split()\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('<end>')\n",
    "    stop_words.add('film')\n",
    "    stop_words.add('movie')\n",
    "\n",
    "    filtered_split_words = [w for w in split_words if not w in stop_words]\n",
    "\n",
    "    try:\n",
    "        top_n_words, top_n_words_count = zip(*Counter(filtered_split_words).most_common(n))\n",
    "    except ValueError:\n",
    "        return [], []\n",
    "    top_n_words_relative_count = np.array(top_n_words_count) / len(filtered_split_words)\n",
    "\n",
    "    return top_n_words, top_n_words_relative_count\n",
    "\n",
    "\n",
    "def pickle_dump_files():\n",
    "    \"\"\"\n",
    "    Saves predictions, state_sentences, decoded_sentences, id,\n",
    "    exemplars, counter exemplars along with top words for instability experiment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'predictions_redone', 'wb') as f:\n",
    "        pickle.dump(predictions, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'state_sentences_redone', 'wb') as f:\n",
    "        pickle.dump(final_state_sentences, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'decoded_sentences_redone', 'wb') as f:\n",
    "        pickle.dump(final_decoded_sentences, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'ids_redone', 'wb') as f:\n",
    "        pickle.dump(idx, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'exemplars_redone', 'wb') as f:\n",
    "        pickle.dump(exemplars, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'counter_exemplars_redone', 'wb') as f:\n",
    "        pickle.dump(counter_exemplars, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'top_exemplar_words_redone', 'wb') as f:\n",
    "        pickle.dump(top_exemplar_words, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'top_counter_exemplar_words_redone', 'wb') as f:\n",
    "        pickle.dump(top_counter_exemplar_words, f)\n",
    "    with open('data/' + dataset_name + '_' + model_name + '_' + 'CF_eval_goal_function_redone', 'wb') as f:\n",
    "        pickle.dump(g_value, f)\n",
    "\n",
    "def create_explanations_csv():\n",
    "    \"\"\"\n",
    "    Creates csv with final explanations along with outher results from X-SPELLS run\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open('output/' + dataset_name + '_' + model_name + '_redone.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(\n",
    "            [\"index\", \"original text\", \"true class\", \"decoded text\", \"black box prediction\",\n",
    "             \"decision tree prediction\", \"fidelity\", \"exemplars\", \"counter exemplars\", \"top exemplar words\",\n",
    "             \"top counter exemplar words\"])\n",
    "        for i in range(len(idx)):\n",
    "            writer.writerow(\n",
    "                [idx[i], X_original[i], y_original[i], final_decoded_sentences[i][0], bbpreds[i], dtpreds[i],\n",
    "                 fidelities[i], exemplars[i], counter_exemplars[i], top_exemplar_words_dict_list[i],\n",
    "                 top_counter_exemplar_words_dict_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize stuff\n",
    "    dataset_name = \"youtube\"\n",
    "    model_name = \"RF\"\n",
    "    pickled_black_box_filename = 'models/' + dataset_name + '_saved_' + model_name + '_model.sav'\n",
    "    \n",
    "    if model_name == \"RF\":\n",
    "        pickled_vectorizer_filename = 'models/' + dataset_name + '_tfidf_vectorizer_redone.pickle'\n",
    "    elif model_name == \"DNN\":\n",
    "        pickled_vectorizer_filename = None\n",
    "\n",
    "    # For how many sentences we want to run X-SPELLS\n",
    "    no_of_sentences = 100\n",
    "    latent_dim = 500\n",
    "    nbr_features = latent_dim\n",
    "    \n",
    "    if dataset_name == \"youtube\":\n",
    "        res = YOUTUBE_train_vae.YOUTUBE_get_text_data(num_samples=20000, data_path='data/YouTube-Spam-Collection-v1/' + dataset_name + '.csv',\n",
    "                                  dataset=dataset_name)\n",
    "    else:\n",
    "        res = train_vae.get_text_data(num_samples=20000, data_path='data/' + dataset_name + '_tweets.csv',\n",
    "                                  dataset=dataset_name)\n",
    "\n",
    "    max_encoder_seq_length, num_enc_tokens, characters, char2id, id2char, \\\n",
    "    encoder_input_data, decoder_input_data, input_texts_original, X_original, y_original, X_original_processed = res\n",
    "    input_dim = encoder_input_data.shape[-1]\n",
    "\n",
    "    vae, enc, gen, stepper = load_VAE(dataset_name)\n",
    "\n",
    "    in_sentences, latent_space_state = get_sentences()\n",
    "    smallest_x, largest_x = calculate_min_max(np.array(latent_space_state))\n",
    "\n",
    "    generated_state_sentences, generated_decoded_sentences = generate_sentences(number_of_sentences=no_of_sentences,\n",
    "                                                                                number_of_max_attempts=5000,\n",
    "                                                                                number_of_random_sentences=600,\n",
    "                                                                                probability=0.4)\n",
    "\n",
    "    predictions, final_state_sentences, final_decoded_sentences = get_predictions(pickled_black_box_filename,\n",
    "                                                                                  pickled_vectorizer_filename,\n",
    "                                                                                  no_of_sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists for later use\n",
    "g_value, idx, fidelities, bbpreds, dtpreds, exemplars, counter_exemplars, top_exemplar_words, top_counter_exemplar_words, \\\n",
    "top_exemplar_words_dict_list, top_counter_exemplar_words_dict_list = ([] for i in range(11))\n",
    "\n",
    "mode = \"DISTANCE\" # DIVERSITY OR DISTANCE\n",
    "\n",
    "print(np.shape(predictions))\n",
    "print(len(predictions))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if np.sum(predictions[i]) == 0:\n",
    "        print(\"heavily inbalanced dataset\")\n",
    "        continue\n",
    "    if np.sum(predictions[i])/len(predictions[i]) == 1:\n",
    "        print(\"heavily inbalanced dataset\")\n",
    "        continue\n",
    "        \n",
    "    print(i)\n",
    "    y = list()\n",
    "\n",
    "    if len(final_decoded_sentences[i]) < 40:\n",
    "        print(len(final_decoded_sentences[i]))\n",
    "        print('Not enough random sentences.')\n",
    "        continue\n",
    "\n",
    "    class_imbalance = False\n",
    "\n",
    "    Z = np.array(final_state_sentences[i]).squeeze()  # convert from 3d to 2d\n",
    "\n",
    "    Z_text = final_decoded_sentences[i]\n",
    "    Yb = np.array(predictions[i])\n",
    "    # pick 100 per label\n",
    "    Z, Z_text, Yb = find_closest_k_latent_sentences(Z, Z_text, Yb, 100)\n",
    "    Z = np.array(Z)\n",
    "    Yb = np.array(Yb)\n",
    "\n",
    "    exemplars_holder = list()\n",
    "    counter_exemplars_holder = list()\n",
    "\n",
    "    Y_0 = (np.count_nonzero(Yb == 0))\n",
    "    Y_1 = (np.count_nonzero(Yb == 1))\n",
    "\n",
    "    # Define as having an imbalance problem when either one of two classes has less than 40% of the total examples\n",
    "    if Y_0 / (Y_0 + Y_1) < 0.4 or Y_1 / (Y_0 + Y_1) < 0.4:\n",
    "        class_imbalance = True\n",
    "\n",
    "    # Catch SMOTE error\n",
    "    if Y_0 < 6 or Y_1 < 6:\n",
    "        print('Not enough samples for smote.')\n",
    "        continue\n",
    "\n",
    "    # If we have class imbalance, apply SMOTE\n",
    "    if class_imbalance:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        Z, Yb = sm.fit_resample(Z, Yb)\n",
    "        print(\"SMOTE USED\")\n",
    "\n",
    "    for t in range(len(Z)):\n",
    "        y.append(np.expand_dims(Z[t], axis=0))  # convert from 2d to 3d\n",
    "\n",
    "    for t in range(len(Z_text), len(Z)):\n",
    "        Z_text.append(decode(y[t]))\n",
    "        \n",
    "    print(np.shape(y))\n",
    "\n",
    "    # Selecting a percentage to test fidelity on the decision tree\n",
    "    indices = np.random.permutation(len(Z))\n",
    "    Z_train_size = 0.95\n",
    "    Z_test_size = 0.05\n",
    "\n",
    "    Z_train, Z_test = Z[indices[:int(len(Z) * Z_train_size)]], Z[indices[int(len(Z) * Z_train_size):]]\n",
    "    Yb_train, Yb_test = Yb[indices[:int(len(Z) * Z_train_size)]], Yb[indices[int(len(Z) * Z_train_size):]]\n",
    "\n",
    "    # Calculate weights\n",
    "    metric = 'euclidean'  # 'euclidean'\n",
    "    kernel_width = float(np.sqrt(nbr_features) * 0.75)\n",
    "    kernel = default_kernel\n",
    "    kernel = partial(kernel, kernel_width=kernel_width)\n",
    "    weights = calculate_weights(Z_train, metric)\n",
    "\n",
    "    # Train latent decision tree\n",
    "    class_values = ['0', '1']\n",
    "    dt = decision_tree.learn_local_decision_tree(Z_train, Yb_train, weights, class_values, prune_tree=False)\n",
    "    Yc = dt.predict(Z)\n",
    "    print('Yc: ', Yc)\n",
    "\n",
    "    # get a list with all elements that belong to the opposite class\n",
    "    opposite_prediction_idx = list()\n",
    "    for t in range(len(Yc)):\n",
    "        # We want the opposite of the instance's prediction\n",
    "        if Yc[0] == 0:\n",
    "            opposite_prediction_idx = np.where(Yc == 1)[0]\n",
    "        else:\n",
    "            opposite_prediction_idx = np.where(Yc == 0)[0]\n",
    "\n",
    "    print('opposite_prediction_idx: ', opposite_prediction_idx)\n",
    "    \n",
    "    # FIND COUNTER EXEMPLARS (indices)\n",
    "    nbr_exemplars = 5\n",
    "    if mode == \"DIVERSITY\":\n",
    "        counter_exemplar_idxs, function_value = DIVERSITY_find_counter_exemplars(latent_space_state[i], Z, opposite_prediction_idx, metric='cosine', count=nbr_exemplars)\n",
    "        #print(function_value)\n",
    "        print(\"final goal value (DIVERSITY)\", function_value[len(function_value) - 1])\n",
    "        function_value = function_value[len(function_value) - 1]\n",
    "    if mode == \"DISTANCE\":\n",
    "        counter_exemplar_idxs = find_counter_exemplars(Z, opposite_prediction_idx, metric='cosine', count=nbr_exemplars)\n",
    "        function_value = EVAL_find_counter_exemplars(latent_space_state[i], Z, opposite_prediction_idx, counter_exemplar_idxs)\n",
    "        print(\"final goal value (DISTANCE)\", function_value)\n",
    "    \n",
    "    print(\"counter_exemplar_idxs:\", counter_exemplar_idxs)\n",
    "\n",
    "    # FIND EXEMPLARS (indices)\n",
    "    # returns the index of the leaf each sample is predicted as\n",
    "    leave_id = dt.apply(Z)\n",
    "    print('leave id: ', leave_id)\n",
    "    \n",
    "    # for the first leave, return all elements that are also in this leaf (their index)\n",
    "    # all elements in this leave are in the same class as the instance we want to explain\n",
    "    others_in_same_leaf = np.where(leave_id == leave_id[0])[0]\n",
    "    print('others in same leaf: ', others_in_same_leaf)\n",
    "    \n",
    "    print('original sentence: ', Z_text[0])\n",
    "\n",
    "    if len(others_in_same_leaf) < nbr_exemplars:\n",
    "        print('Not enough exemplars in the leaf, will find by distance instead...', len(others_in_same_leaf))\n",
    "        same_prediction_idx = list()\n",
    "        for t in range(1, len(Yc)):\n",
    "            # We want the same as the instance's prediction\n",
    "            if Yc[0] == 0:\n",
    "                same_prediction_idx = np.where(Yc == 0)[0]\n",
    "            else:\n",
    "                same_prediction_idx = np.where(Yc == 1)[0]\n",
    "\n",
    "        unique_exemplars = list(set(find_exemplars(Z, same_prediction_idx, metric='cosine')))\n",
    "        print(unique_exemplars)\n",
    "        selected_exemplars = unique_exemplars[:nbr_exemplars]\n",
    "    else:\n",
    "        # choose a random subset of 5 sentences from the \"target leaf\"\n",
    "        selected_exemplars = np.random.choice(others_in_same_leaf, size=nbr_exemplars, replace=False)\n",
    "\n",
    "    number_of_words = 5\n",
    "    \n",
    "    print(\"-----\")\n",
    "    print('exemplars:')\n",
    "    print(\"-----\")\n",
    "    \n",
    "    for j in selected_exemplars:\n",
    "        print(Z_text[j])\n",
    "        exemplars_holder.append(Z_text[j])\n",
    "\n",
    "    np_exemplars_holder = np.array(exemplars_holder)\n",
    "    top_n_exemplar_words, top_n_exemplar_words_relative_count = \\\n",
    "        find_most_common_words(np_exemplars_holder, number_of_words)\n",
    "\n",
    "    print(top_n_exemplar_words)\n",
    "    print(top_n_exemplar_words_relative_count)\n",
    "\n",
    "    top_exemplar_words_dict = dict(zip(top_n_exemplar_words, top_n_exemplar_words_relative_count))\n",
    "\n",
    "    print(top_exemplar_words_dict)\n",
    "\n",
    "    print(\"-----\")\n",
    "    print('counter exemplars:')\n",
    "    print(\"-----\")\n",
    "    \n",
    "    for j in counter_exemplar_idxs:\n",
    "        print(Z_text[j])\n",
    "        counter_exemplars_holder.append(Z_text[j])\n",
    "\n",
    "    np_counter_exemplars_holder = np.array(counter_exemplars_holder)\n",
    "    top_n_counter_exemplar_words, top_n_counter_exemplar_words_relative_count = \\\n",
    "        find_most_common_words(np_counter_exemplars_holder, number_of_words)\n",
    "\n",
    "    print(top_n_counter_exemplar_words)\n",
    "    print(top_n_counter_exemplar_words_relative_count)\n",
    "\n",
    "    top_counter_exemplar_words_dict = dict(zip(top_n_counter_exemplar_words,\n",
    "                                               top_n_counter_exemplar_words_relative_count))\n",
    "\n",
    "    print(top_counter_exemplar_words_dict)\n",
    "    print('original sentence', X_original[i])\n",
    "    print('true class', y_original[i])\n",
    "    print('black box prediction', Yb[0])\n",
    "    print('decision tree prediction', Yc[0])\n",
    "\n",
    "    fidelity = accuracy_score(Yb, Yc)\n",
    "    print('fidelity', fidelity)\n",
    "    \n",
    "    if len(counter_exemplars_holder) < 5:\n",
    "        print(\"less than 5 CF\")\n",
    "        continue\n",
    "\n",
    "    idx.append(i)\n",
    "    fidelities.append(fidelity)\n",
    "    bbpreds.append(Yb[0])\n",
    "    dtpreds.append(Yc[0])\n",
    "    exemplars.append(exemplars_holder)\n",
    "    counter_exemplars.append(counter_exemplars_holder)\n",
    "    top_exemplar_words.append(top_n_exemplar_words)\n",
    "    top_counter_exemplar_words.append(top_n_counter_exemplar_words)\n",
    "    top_exemplar_words_dict_list.append(top_exemplar_words_dict)\n",
    "    top_counter_exemplar_words_dict_list.append(top_counter_exemplar_words_dict)\n",
    "    g_value.append(function_value)\n",
    "    print('')\n",
    "    \n",
    "pickle_dump_files()\n",
    "create_explanations_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env_3 clone",
   "language": "python",
   "name": "clone_xspells_conda_env_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
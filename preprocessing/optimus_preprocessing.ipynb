{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for OPTIMUS\n",
    "\n",
    "We partition the data as follows: 75%/25% > 75%/25%\n",
    "\n",
    "75% of the latter are used for fine-tuning the pretrained OPTIMUS VAE\n",
    "\n",
    "25% are used to evaluate the fine-tuned VAE (automatically) and to generate explanations\n",
    "\n",
    ".data files for sentence generation\n",
    "\n",
    ".txt files for VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/laura/miniconda3/envs/xspells_conda_env_3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lstm_vae import create_lstm_vae, inference\n",
    "from pre_processing import preProcessing, YOUTUBE_preProcessing\n",
    "\n",
    "import sklearn\n",
    "from scipy.spatial.distance import cdist\n",
    "from statistics import stdev\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPTIMUS_get_text_data(num_samples, data_path, dataset):\n",
    "    thousandwords = [line.rstrip('\\n') for line in open('data/1-1000.txt')]\n",
    "\n",
    "    # vectorize the data\n",
    "    input_texts = []\n",
    "    input_texts_test = []\n",
    "    input_texts_original = []\n",
    "    input_texts_original_test = []\n",
    "    \n",
    "    input_words = set([\"\\t\"])\n",
    "    all_input_words = []\n",
    "    \n",
    "    lines = []\n",
    "    lines_test = []\n",
    "    \n",
    "    df = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "    if dataset == \"polarity\":\n",
    "        X = df['tweet'].values\n",
    "        y = df['class'].values\n",
    "    elif dataset == \"hate\":\n",
    "        # Removing the offensive comments, keeping only neutral and hatespeech,\n",
    "        # and convert the class value from 2 to 1 for simplification purposes\n",
    "        df = df[df['class'] != 1]\n",
    "        X = df['tweet'].values\n",
    "        y = df['class'].apply(lambda x: 1 if x == 2 else 0).values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.25)\n",
    "    \n",
    "    # add another split of the data set\n",
    "    # parameter stratify: preserves class-relations in data set    \n",
    "    X_train_subsplit, X_test_subsplit, y_train_subsplit, y_test_subsplit = train_test_split(X_test, y_test, random_state=42, stratify=y_test, test_size=0.25)\n",
    "\n",
    "    new_X_train_subsplit = preProcessing(X_train_subsplit)\n",
    "    new_X_test_subsplit = preProcessing(X_test_subsplit)\n",
    "    \n",
    "    # clean training set\n",
    "    for line in new_X_train_subsplit:\n",
    "        input_texts_original.append(line)\n",
    "        # lowercase and remove punctuation\n",
    "        lines.append(line.lower().translate(str.maketrans('', '', string.punctuation)))  \n",
    "        \n",
    "    # clean test set\n",
    "    for line in new_X_test_subsplit:\n",
    "        input_texts_original_test.append(line)\n",
    "        # lowercase and remove punctuation\n",
    "        lines_test.append(line.lower().translate(str.maketrans('', '', string.punctuation)))  \n",
    "\n",
    "    return input_texts_original, y_train_subsplit, input_texts_original_test, y_test_subsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOUTUBE_OPTIMUS_get_text_data(num_samples, data_path, dataset):\n",
    "    thousandwords = [line.rstrip('\\n') for line in open('data/1-1000.txt')]\n",
    "\n",
    "    # vectorize the data\n",
    "    input_texts = []\n",
    "    input_texts_test = []\n",
    "    input_texts_original = []\n",
    "    input_texts_original_test = []\n",
    "    \n",
    "    input_words = set([\"\\t\"])\n",
    "    all_input_words = []\n",
    "    \n",
    "    lines = []\n",
    "    lines_test = []\n",
    "        \n",
    "    df = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "    X = df[\"CONTENT\"].values\n",
    "    y = df[\"CLASS\"].values\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.25)\n",
    "    X_train_subsplit, X_test_subsplit, y_train_subsplit, y_test_subsplit = train_test_split(X_test, y_test, random_state=42, stratify=y_test, test_size=0.25)\n",
    "    \n",
    "    print(len(y_train_subsplit))\n",
    "    print(len(y_test_subsplit))\n",
    "    \n",
    "    new_X_test_subsplit = YOUTUBE_preProcessing(X_test_subsplit)\n",
    "    new_X_train_subsplit = YOUTUBE_preProcessing(X_train_subsplit)\n",
    "    \n",
    "    # delete x/y where there is no more content after preprocessing (e.g. comment was only an url)\n",
    "    \n",
    "    indx = []\n",
    "    for i in range(len(new_X_test_subsplit)):\n",
    "        if len(new_X_test_subsplit[i]) == 0:\n",
    "            indx.append(i)\n",
    "        elif len(new_X_test_subsplit[i]) > 140:\n",
    "            indx.append(i)     \n",
    "    new_X_test_subsplit = np.delete(new_X_test_subsplit, indx, 0)\n",
    "    y_test_subsplit = np.delete(y_test_subsplit, indx, 0)\n",
    "    \n",
    "    indx_train = []\n",
    "    for i in range(len(new_X_train_subsplit)):\n",
    "        if len(new_X_train_subsplit[i]) == 0:\n",
    "            indx_train.append(i)\n",
    "        if len(new_X_train_subsplit[i]) > 140:\n",
    "            indx_train.append(i)\n",
    "    new_X_train_subsplit = np.delete(new_X_train_subsplit, indx_train, 0)\n",
    "    y_train_subsplit = np.delete(y_train_subsplit, indx_train, 0)\n",
    "    \n",
    "    # clean training set\n",
    "    for line in new_X_train_subsplit:\n",
    "        input_texts_original.append(line)\n",
    "        # lowercase and remove punctuation\n",
    "        lines.append(line.lower().translate(str.maketrans('', '', string.punctuation)))  \n",
    "        \n",
    "    # clean test set\n",
    "    for line in new_X_test_subsplit:\n",
    "        input_texts_original_test.append(line)\n",
    "        # lowercase and remove punctuation\n",
    "        lines_test.append(line.lower().translate(str.maketrans('', '', string.punctuation)))  \n",
    "\n",
    "    return input_texts_original, y_train_subsplit, input_texts_original_test, y_test_subsplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_name = 'polarity'\n",
    "\n",
    "res = OPTIMUS_get_text_data(num_samples=20000, data_path='data/' + dataset_name + '_tweets.csv', dataset=dataset_name)\n",
    "input_texts, y_train, input_texts_test, y_test = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe data and labels\n",
    "# input sentence generation is .data\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(dataset_name +'_training_data.data', 'wb') as filehandle:\n",
    "    pickle.dump(input_texts, filehandle)\n",
    "\n",
    "#with open(dataset_name +'_test_data.data', 'wb') as filehandle:\n",
    "#    pickle.dump(input_texts_test, filehandle)\n",
    "    \n",
    "#with open(dataset_name +'_training_labels.data', 'wb') as filehandle:\n",
    "#    pickle.dump(y_train, filehandle)\n",
    "\n",
    "#with open(dataset_name +'_test_labels.data', 'wb') as filehandle:\n",
    "#    pickle.dump(y_test, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe data\n",
    "# input VAE training is txt\n",
    "\n",
    "#file = open(dataset_name + \"_training_data.txt\", \"w\")\n",
    "\n",
    "#for i in range(len(input_texts)):\n",
    "#    file.write(input_texts[i])\n",
    "#    file.write(\"\\n\")\n",
    "#file.close() \n",
    "\n",
    "#file = open(dataset_name + \"_test_data.txt\", \"w\")\n",
    "\n",
    "#for i in range(len(input_texts_test)):\n",
    "#    file.write(input_texts_test[i])\n",
    "#    file.write(\"\\n\")\n",
    "#file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env 3",
   "language": "python",
   "name": "xspells_conda_env_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
